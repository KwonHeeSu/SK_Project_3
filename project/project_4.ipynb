{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b7e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e43550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec05879",
   "metadata": {},
   "source": [
    "### Project 4-1. OpenAI에서 Ollama Qwen3로 RAG 시스템 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cc40f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 1. 문서 로딩 → PDF 읽기...\n",
      "  총 39페이지 로드 완료\n",
      "==> 2. 문서 분할 → 작은 청크로 나누기\n",
      "  76개 청크 생성 완료\n",
      "==> 3. 벡터화 → 임베딩으로 변환\n",
      "==> 4. 저장 → FAISS 벡터스토어에 저장\n",
      "===> 5. 검색 → 질문과 유사한 문서 찾기\n",
      "===> 6. 생성 → LLM으로 답변 생성\n",
      "<think>\n",
      "Okay, let's see. The user is asking about the characteristics of the RAG system rewritten with Qwen3. But wait, the context provided is about BlueJ, a Java development environment, and some installation instructions. There's no mention of RAG or Qwen3 in the given text.\n",
      "\n",
      "So, the user might be referring to a different topic. The context is about programming tutorials, BlueJ, Java classes, compiling, and installing BlueJ. There's no information about RAG (Retrieval-Augmented Generation) systems or Qwen3. Since the question is outside the provided context, I should inform the user that I don't have the necessary information to answer their question.\n",
      "</think>\n",
      "\n",
      "I don't know the answer to this question because the provided context is about BlueJ, a Java development environment, and does not mention anything related to RAG systems or Qwen3. The information given is focused on programming tutorials, class implementation, and BlueJ installation, which are unrelated to the question asked.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"==> 1. 문서 로딩 → PDF 읽기...\")\n",
    "loader = PyPDFLoader('../data/tutorial-korean.pdf')\n",
    "documents = loader.load()\n",
    "print(f\"  총 {len(documents)}페이지 로드 완료\")\n",
    "\n",
    "print(\"==> 2. 문서 분할 → 작은 청크로 나누기\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,        # 로컬 모델에 맞게 조금 더 작게!\n",
    "    chunk_overlap=150,     # 중복 조금만 남기기\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"  {len(chunks)}개 청크 생성 완료\")\n",
    "\n",
    "print(\"==> 3. 벡터화 → 임베딩으로 변환\")\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={'device': 'cpu'},  # GPU면 'cuda'\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"==> 4. 저장 → FAISS 벡터스토어에 저장\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"===> 5. 검색 → 질문과 유사한 문서 찾기\")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # 최적화: 4개만 반환\n",
    ")\n",
    "\n",
    "print(\"===> 6. 생성 → LLM으로 답변 생성\")\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:1.7b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    num_predict=1500\n",
    ")\n",
    "\n",
    "# ↓↓↓ RAG QA 체인 구성 및 사용 예시 ↓↓↓\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "question = \"Qwen3로 바꾼 RAG 시스템의 특징은?\"\n",
    "result = qa({\"query\": question})\n",
    "\n",
    "print(result[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
